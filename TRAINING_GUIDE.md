# Quran ASR Training Summary

## âœ… Complete Setup Accomplished

### 1. **Dataset Preparation** 
- âœ… Downloaded Quran dataset from HuggingFace (74,889 samples)
- âœ… Fixed AudioDecoder handling from torchcodec library
- âœ… Resampled all audio to 16 kHz (NeMo requirement)
- âœ… Split into train (67,399), val (3,745), test (3,745)
- âœ… Generated NeMo-compatible JSONL manifests

**Location**: `data/manifests/` and `data/audio/`

### 2. **Tokenizer Creation**
- âœ… Trained SentencePiece BPE tokenizer on full corpus
- âœ… Vocabulary size: 1,024 tokens
- âœ… Special tokens: `<s>` (BOS), `</s>` (EOS)
- âœ… Character coverage: 99.95% (optimized for Arabic)

**Location**: `tokenizer/tokenizer_spe_bpe_v1024_bos_eos/`

### 3. **Model Configuration**
- âœ… Using FastConformer Hybrid (RNNT-CTC) architecture
- âœ… 114M parameters, streaming-capable
- âœ… Batch size: 8 (optimized for memory)
- âœ… Multi-worker DataLoader disabled for stability

**Config**: `nemo_scripts/fastconformer_hybrid_transducer_ctc_bpe_streaming.yaml`

### 4. **Training Status**
- âœ… Training script (`train_nemo.sh`) is functional
- âœ… Model initializes correctly with prepared data
- âœ… Achieved ~3.5 iterations/second throughput
- âœ… Checkpoint saving works properly

**Test Run Results**: 
- Processed 569 batches (8% of epoch)
- WER computed on validation data
- Model checkpoint saved automatically

## ğŸš€ Running Full Training

To train for multiple epochs:

```bash
# Activate environment
conda activate quranASR

# Run training (10 epochs by default)
cd /data/SAB_PhD/quranNemoASR
bash train_nemo.sh

# Or with custom parameters:
TOKENIZER_DIR=$(pwd)/tokenizer/tokenizer_spe_bpe_v1024_bos_eos bash train_nemo.sh
```

**Training Parameters** (see `train_nemo.sh`):
- Max epochs: 10
- Batch size: 8
- Learning rate: default (0.001)
- Log interval: every 5 steps
- GPU devices: 1 (configurable)

## ğŸ“Š Expected Output

Training logs and checkpoints are saved to:
```
nemo_experiments/FastConformer-Hybrid-Transducer-CTC-BPE-Streaming/
â””â”€â”€ <timestamp>/
    â”œâ”€â”€ checkpoints/
    â”‚   â””â”€â”€ FastConformer-*.nemo
    â”œâ”€â”€ hparams.yaml
    â”œâ”€â”€ nemo_log_globalrank-0_localrank-0.txt
    â””â”€â”€ events.out.tfevents.* (TensorBoard logs)
```

## âš™ï¸ Advanced Configuration

To modify training parameters, edit `train_nemo.sh` or pass overrides:

```bash
# Example: 20 epochs, larger batch size, custom learning rate
bash train_nemo.sh trainer.max_epochs=20 model.train_ds.batch_size=16 model.optim.lr=0.0005
```

## ğŸ”§ Troubleshooting

### Out of Memory (OOM)
- Reduce batch size: `model.train_ds.batch_size=4`
- Reduce max duration: `model.train_ds.max_duration=15`

### Slow Training
- Increase batch size (if memory allows)
- Use CPU workers: already set to `num_workers=0`
- Enable mixed precision: `trainer.precision=16-mixed`

### Data Loading Issues
- Ensure audio files exist in `data/audio/`
- Verify manifests are valid JSON lines format
- Check file permissions: `chmod +r data/audio/*`

## ğŸ“ˆ Next Steps

1. **Monitor training**: Check `nemo_experiments/` directory for logs
2. **Evaluate**: Use validation split for performance monitoring
3. **Fine-tune**: Adjust hyperparameters based on WER metrics
4. **Export**: Convert checkpoint to inference-ready format

---

**Pipeline Status**: âœ… Ready for production training
**Last Updated**: 2026-02-12
**Git Status**: All changes committed
